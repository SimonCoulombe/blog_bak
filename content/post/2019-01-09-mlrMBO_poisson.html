---
title: "Bayesian optimization of xgboost poisson regression in R"
author: "Simon Coulombe"
date: 2019-01-09
slug: "bayesian"
output:
  html_document:
    code_folding: hide
    number_sections: true
    theme: simplex
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    editor_options:   
      chunk_output_type: console
categories: ["R"]
tags: ["bayesian", "optimization", "rstats", "mlrMBO", "poisson", "rBayesianOptimization"]
---



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In this document, I will show how to optimize the xgboost hyperparameters for a poisson regression in R using the <code>mlrMBO</code> package. We will model the number of auto insurance claims based on characteristics of the car and driver, while offsetting for exposure. The data comes from the <code>insuranceData</code> package.</p>
<p>We will demonstrate the code for a random search, a bayesian optimization using the <code>mlrMBO</code>package and also show how to perform a bayesian optimization using the deprecated (?) <code>rBayesianOptimization</code> package.</p>
<p>The <code>mlrMBO</code> package appears to be well maintained. It is normally used in combination with the <code>mlr</code> package, but I couldnt find a way to use <code>mlr</code> to model a poisson regression using xgboost.</p>
<p>I also tried the <code>rBayesianOptimization</code> package after being inspired by this post from <a href="http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html">Max Kuhn</a> in 2016 the example from <code>rBayesianOptimization</code> ’s help. I do not recommend using this package because it <a href="https://github.com/yanyachen/rBayesianOptimization/issues/4">sometimes recycles hyperparameters</a> and hasnt been updated on github since 2016.</p>
<p>In this document, we will use the , <code>xgboost</code>, <code>tidyverse</code>, <code>mlrMBO</code>, <code>insuranceData</code> and <code>rBayesianOptimization</code> packages.</p>
<pre class="r"><code>library(xgboost)
library(insuranceData) # example dataset https://cran.r-project.org/web/packages/insuranceData/insuranceData.pdf
library(tidyverse) # for data wrangling
library(rBayesianOptimization) # to create cv folds and for bayesian optimisation
library(mlrMBO)  # for bayesian optimisation
library(DataExplorer) # for summarising databases
library(skimr) # for summarising databases
library(purrr) # to evaluate the loglikelihood of each parameter set in the random grid search</code></pre>
</div>
<div id="preparing-the-data" class="section level1">
<h1>Preparing the data</h1>
<p>First, we load the dataCar data from the <code>insuranceData</code> package. It contains 67856 one-year vehicle insurance policies taken out in 2004 or 2005.</p>
<p>The dependent variable is <code>numclaims</code>, which represents the number of claims.</p>
<p>The <code>exposure</code> variable represents the “number of year of exposure” and is used as the offset variable. Its value is bounded between 0 and 1.</p>
<p>Finally, the independent variables are as follow:</p>
<ul>
<li><code>veh_value</code>, the vehicle value in tens of thousand of dollars,<br />
</li>
<li><code>veh_body</code>, y vehicle body, coded as BUS CONVT COUPE HBACK HDTOP MCARA MIBUS PANVN RDSTR SEDAN STNWG TRUCK UTE,<br />
</li>
<li><code>veh_age</code>, 1 (youngest), 2, 3, 4,<br />
</li>
<li><code>gender</code>, a factor with levels F M,<br />
</li>
<li><code>area</code> a factor with levels A B C D E F,<br />
</li>
<li><code>agecat</code> 1 (youngest), 2, 3, 4, 5, 6</li>
</ul>
<p>Insurance ratemaking often requires monotonous relationships. In our case, we will arbitrarily force the number of claims to be non-increasing with the age of the vehicle.</p>
<p>Below, we import the data, one-hot encode dummy variables for xgboost consumption, set the offset, set the constraints and define the 3 folds for cross-validation.</p>
<pre class="r"><code># load insurance data
data(dataCar)
mydb &lt;- dataCar %&gt;% select(numclaims, exposure, veh_value, veh_body,
                           veh_age, gender, area, agecat)

label_var &lt;- &quot;numclaims&quot;  
offset_var &lt;- &quot;exposure&quot;
feature_vars &lt;- mydb %&gt;% 
  select(-one_of(c(label_var, offset_var))) %&gt;% 
  colnames()

skimr::skim(mydb ) %&gt;% 
  skimr::kable()</code></pre>
<pre><code>## Skim summary statistics  
##  n obs: 67856    
##  n variables: 8    
## 
## Variable type: factor
## 
##  variable    missing    complete      n      n_unique                     top_counts                      ordered 
## ----------  ---------  ----------  -------  ----------  -----------------------------------------------  ---------
##    area         0        67856      67856       6            C: 20540, A: 16312, B: 13341, D: 8173         FALSE  
##   gender        0        67856      67856       2                  F: 38603, M: 29253, NA: 0               FALSE  
##  veh_body       0        67856      67856       13       SED: 22233, HBA: 18915, STN: 16261, UTE: 4586     FALSE  
## 
## Variable type: integer
## 
##  variable     missing    complete      n      mean      sd     p0    p25    p50    p75    p100      hist   
## -----------  ---------  ----------  -------  -------  ------  ----  -----  -----  -----  ------  ----------
##   agecat         0        67856      67856    3.49     1.43    1      2      3      5      6      ▃▆▁▇▇▁▅▃ 
##  numclaims       0        67856      67856    0.073    0.28    0      0      0      0      4      ▇▁▁▁▁▁▁▁ 
##   veh_age        0        67856      67856    2.67     1.07    1      2      3      4      4      ▅▁▆▁▁▇▁▇ 
## 
## Variable type: numeric
## 
##  variable     missing    complete      n      mean     sd       p0      p25     p50     p75     p100       hist   
## -----------  ---------  ----------  -------  ------  ------  --------  ------  ------  ------  -------  ----------
##  exposure        0        67856      67856    0.47    0.29    0.0027    0.22    0.45    0.71      1      ▇▇▇▇▆▆▆▆ 
##  veh_value       0        67856      67856    1.78    1.21      0       1.01    1.5     2.15    34.56    ▇▁▁▁▁▁▁▁</code></pre>
<pre class="r"><code># one hot encoding of categorical (factor) data
myformula &lt;- paste0( &quot;~&quot;, paste0( feature_vars, collapse = &quot; + &quot;) ) %&gt;% as.formula()
dummyFier &lt;- caret::dummyVars(myformula, data=mydb, fullRank = TRUE)
dummyVars.df &lt;- predict(dummyFier,newdata = mydb)
mydb_dummy &lt;- cbind(mydb %&gt;% select(one_of(c(label_var, offset_var))), 
                    dummyVars.df)
rm(myformula, dummyFier, dummyVars.df)


# get  list the column names of the db with the dummy variables
feature_vars_dummy &lt;-  mydb_dummy  %&gt;% select(-one_of(c(label_var, offset_var))) %&gt;% colnames()

# create xgb.matrix for xgboost consumption
mydb_xgbmatrix &lt;- xgb.DMatrix(
  data = mydb_dummy %&gt;% select(feature_vars_dummy) %&gt;% as.matrix, 
  label = mydb_dummy %&gt;% pull(label_var),
  missing = &quot;NAN&quot;)


#base_margin: apply exposure offset 
setinfo(mydb_xgbmatrix,&quot;base_margin&quot;, mydb %&gt;% pull(offset_var) %&gt;% log() )</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code># a fake constraint, just to show how it is done.  Here we force &quot;the older the car, the less likely are claims&quot;
myConstraint   &lt;- data_frame(Variable = feature_vars_dummy) %&gt;%
  mutate(sens = ifelse(Variable == &quot;veh_age&quot;, -1, 0))

# random folds for xgb.cv
cv_folds = rBayesianOptimization::KFold(mydb_dummy$numclaims,
                                        nfolds= 3,
                                        stratified = TRUE,
                                        seed= 0)</code></pre>
</div>
<div id="example-1-optimize-hyperparameters-using-a-random-search-non-bayesian" class="section level1">
<h1>Example 1: Optimize hyperparameters using a random search (non bayesian)</h1>
<p>We will start with a quick example of random search.</p>
<p>I don’t use caret for the random search <a href="https://github.com/topepo/caret/issues/861">because it has a hard time with poisson regression</a>.</p>
<p>I will generate 20 random sets of hyperparameters. I will force the gamma’s half the random sets to be equal to 0. I will also hardcode an extra set of parameters named <code>simon_params</code> because they are often a good starting point.</p>
<pre class="r"><code># generate hard coded parameters
simon_params &lt;- data.frame(max_depth = 6,
                           colsample_bytree= 0.8,
                           subsample = 0.8,
                           min_child_weight = 3,
                           eta  = 0.01,
                           gamma = 0,
                           nrounds = 200) %&gt;% as_tibble()

# generate 20 random models
how_many_models &lt;- 20
max_depth &lt;-data.frame(max_depth = floor(runif(how_many_models)*5 )+3)  # 1 à 4
colsample_bytree &lt;- data.frame(colsample_bytree =runif(how_many_models)*0.8 +0.2)  # 0.2 à 1
subsample &lt;- data.frame(subsample =runif(how_many_models)*0.8 +0.2) # 0.2 à 1
min_child_weight &lt;- data.frame(min_child_weight = floor(runif(how_many_models)*10 )+1) # 1 à 10
eta &lt;- data.frame(eta = runif(how_many_models)*0.06 +0.002) # 0.002 à 0.062
gamma &lt;-  data.frame(gamma =c(rep(0,how_many_models/2), runif(how_many_models/2)*10)) # 0 à 10
nrounds &lt;- data.frame(nrounds = rep(2e2,how_many_models)) # max 200

random_grid &lt;-max_depth %&gt;%
  bind_cols(colsample_bytree ) %&gt;%
  bind_cols(subsample) %&gt;%
  bind_cols(min_child_weight) %&gt;%
  bind_cols(eta) %&gt;%
  bind_cols(gamma) %&gt;%
  bind_cols(nrounds)  %&gt;% as_tibble()

# combine random and hardcoded parameters
df.params &lt;- 
  simon_params %&gt;%  bind_rows(random_grid) %&gt;%
  mutate(rownum = row_number(),
         rownumber = row_number())


list_of_param_sets &lt;- df.params %&gt;% nest(-rownum)</code></pre>
<p>Here are the hyperparameters that will be tested:</p>
<pre class="r"><code>kable(df.params)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">max_depth</th>
<th align="center">colsample_bytree</th>
<th align="center">subsample</th>
<th align="center">min_child_weight</th>
<th align="center">eta</th>
<th align="center">gamma</th>
<th align="center">nrounds</th>
<th align="center">rownum</th>
<th align="center">rownumber</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8000000</td>
<td align="center">0.8000000</td>
<td align="center">3</td>
<td align="center">0.0100000</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">0.3697140</td>
<td align="center">0.7176482</td>
<td align="center">3</td>
<td align="center">0.0447509</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">0.7213390</td>
<td align="center">0.8263462</td>
<td align="center">5</td>
<td align="center">0.0259997</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.3004441</td>
<td align="center">0.6424290</td>
<td align="center">4</td>
<td align="center">0.0215211</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">4</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.4137765</td>
<td align="center">0.6237757</td>
<td align="center">7</td>
<td align="center">0.0474252</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">5</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.5088913</td>
<td align="center">0.8314850</td>
<td align="center">3</td>
<td align="center">0.0141615</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">6</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">0.2107123</td>
<td align="center">0.2186650</td>
<td align="center">5</td>
<td align="center">0.0446673</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">7</td>
<td align="center">7</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">0.5059104</td>
<td align="center">0.5817841</td>
<td align="center">8</td>
<td align="center">0.0093015</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">8</td>
<td align="center">8</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8957527</td>
<td align="center">0.7858510</td>
<td align="center">1</td>
<td align="center">0.0167293</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">9</td>
<td align="center">9</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0.4722792</td>
<td align="center">0.7541852</td>
<td align="center">9</td>
<td align="center">0.0105983</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">10</td>
<td align="center">10</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.5856641</td>
<td align="center">0.5820957</td>
<td align="center">4</td>
<td align="center">0.0163778</td>
<td align="center">0.000000</td>
<td align="center">200</td>
<td align="center">11</td>
<td align="center">11</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0.6796527</td>
<td align="center">0.8889676</td>
<td align="center">9</td>
<td align="center">0.0055361</td>
<td align="center">3.531973</td>
<td align="center">200</td>
<td align="center">12</td>
<td align="center">12</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.5948330</td>
<td align="center">0.5504777</td>
<td align="center">4</td>
<td align="center">0.0405373</td>
<td align="center">2.702602</td>
<td align="center">200</td>
<td align="center">13</td>
<td align="center">13</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">0.3489741</td>
<td align="center">0.3958378</td>
<td align="center">4</td>
<td align="center">0.0545762</td>
<td align="center">9.926841</td>
<td align="center">200</td>
<td align="center">14</td>
<td align="center">14</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8618987</td>
<td align="center">0.2565432</td>
<td align="center">5</td>
<td align="center">0.0487349</td>
<td align="center">6.334933</td>
<td align="center">200</td>
<td align="center">15</td>
<td align="center">15</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="center">0.7347734</td>
<td align="center">0.2795729</td>
<td align="center">9</td>
<td align="center">0.0498385</td>
<td align="center">2.132081</td>
<td align="center">200</td>
<td align="center">16</td>
<td align="center">16</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8353919</td>
<td align="center">0.4530174</td>
<td align="center">9</td>
<td align="center">0.0293165</td>
<td align="center">1.293724</td>
<td align="center">200</td>
<td align="center">17</td>
<td align="center">17</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.2863549</td>
<td align="center">0.6149074</td>
<td align="center">4</td>
<td align="center">0.0266050</td>
<td align="center">4.781180</td>
<td align="center">200</td>
<td align="center">18</td>
<td align="center">18</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.7789688</td>
<td align="center">0.7296041</td>
<td align="center">8</td>
<td align="center">0.0506522</td>
<td align="center">9.240745</td>
<td align="center">200</td>
<td align="center">19</td>
<td align="center">19</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">0.5290195</td>
<td align="center">0.5254641</td>
<td align="center">10</td>
<td align="center">0.0382960</td>
<td align="center">5.987610</td>
<td align="center">200</td>
<td align="center">20</td>
<td align="center">20</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">0.8567570</td>
<td align="center">0.9303007</td>
<td align="center">5</td>
<td align="center">0.0412834</td>
<td align="center">9.761707</td>
<td align="center">200</td>
<td align="center">21</td>
<td align="center">21</td>
</tr>
</tbody>
</table>
<p>Evaluate these 21 models with 3 folds and retourn the loglikelihood of each models:</p>
<pre class="r"><code>start &lt;- Sys.time()

random_grid_results &lt;- list_of_param_sets %&gt;% 
  mutate(booster = map(data, function(X){
    message(paste0(&quot;model #&quot;, X$rownumber), 
            &quot; eta = &quot;, X$eta,
            &quot; max.depth = &quot;, X$max_depth, 
            &quot; min_child_weigth = &quot;, X$min_child_weight,
            &quot; subsample = &quot;, X$subsample, 
            &quot; colsample_bytree = &quot;, X$colsample_bytree, 
            &quot;, gamma = &quot;,  X$gamma,&quot; , 
            nrounds = &quot;, X$nrounds)
    set.seed(1234)
    
    cv &lt;- xgb.cv(params = list(
      booster = &quot;gbtree&quot;,
      eta = X$eta,
      max_depth = X$max_depth,
      min_child_weight = X$min_child_weight,
      gamma = X$gamma,
      subsample = X$subsample,
      colsample_bytree = X$colsample_bytree,
      objective = &#39;count:poisson&#39;, 
      eval_metric = &quot;poisson-nloglik&quot;),
      data = mydb_xgbmatrix,
      nround = X$nrounds,
      folds=  cv_folds,
      monotone_constraints = myConstraint$sens,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 50,
      verbose = 0)
    
    function_return &lt;- 
      list(
        Score = cv$evaluation_log[, max(test_poisson_nloglik_mean)], # l&#39;itération qui a la plus haute moyenne de nloglik sur toutes les cv folds
        Pred = 0)
    message(paste0(&quot;Score :&quot;, function_return$Score))
    return(function_return)})) %&gt;%
  mutate(Score =  pmap(list(booster), function(X){X$Score })%&gt;% unlist())

write_rds(random_grid_results, &quot;random_grid_results.rds&quot;)
stop &lt;- Sys.time()
stop- start # 4 minutes on 32 cores</code></pre>
<pre class="r"><code>random_grid_results &lt;- read_rds( &quot;random_grid_results.rds&quot;)

#kable(random_grid_results)

random_grid_results %&gt;%
  mutate( hardcoded = ifelse(rownum ==1,TRUE,FALSE)) %&gt;%
  ggplot(aes( x = rownum, y = Score, color = hardcoded)) + 
  geom_point() +
  labs(title = &quot;random grid search&quot;)+
  ylab(&quot;loglikelihood&quot;)</code></pre>
<p><img src="/post/2019-01-09-mlrMBO_poisson_files/figure-html/unnamed-chunk-4-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="example-2-bayesian-optimization-using-mlrmbo" class="section level1">
<h1>Example #2 Bayesian optimization using <code>mlrMBO</code></h1>
<p>This tutorial builds on the <a href="https://mlrmbo.mlr-org.com/articles/mlrMBO.html">mlrMBO vignette</a></p>
<p>First, we need to define the objective function that the bayesian search will try to maximise. In this case we want to maximise the log likelihood of the out of fold predictions.</p>
<pre class="r"><code># objective function: we want to maximise the log likelihood by tuning most parameters
obj.fun  &lt;- smoof::makeSingleObjectiveFunction(
  name = &quot;xgb_cv_bayes&quot;,
  fn =   function(x){
    set.seed(1234)
    cv &lt;- xgb.cv(params = list(
      booster = &quot;gbtree&quot;,
      eta = x[&quot;eta&quot;],
      max_depth = x[&quot;max_depth&quot;],
      min_child_weight = x[&quot;min_child_weight&quot;],
      gamma = x[&quot;gamma&quot;],
      subsample =x[&quot;subsample&quot;],
      colsample_bytree = x[&quot;colsample_bytree&quot;],
      objective = &#39;count:poisson&#39;, 
      eval_metric = &quot;poisson-nloglik&quot;),
      data = mydb_xgbmatrix,
      nround = 30,
      folds=  cv_folds,
      monotone_constraints = myConstraint$sens,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 10,
      verbose = 0)
    
    cv$evaluation_log[, max(test_poisson_nloglik_mean)]
  },
  par.set = makeParamSet(
    makeNumericParam(&quot;eta&quot;, lower = 0.001, upper = 0.05),
    makeNumericParam(&quot;gamma&quot;, lower = 0, upper = 5),
    makeIntegerParam(&quot;max_depth&quot;, lower= 1, upper = 10),
    makeIntegerParam(&quot;min_child_weight&quot;, lower= 1, upper = 10),
    makeNumericParam(&quot;subsample&quot;, lower = 0.2, upper = 1),
    makeNumericParam(&quot;colsample_bytree&quot;, lower = 0.2, upper = 1)
  ),
  minimize = FALSE
)

# generate an optimal design with only 10  points
des = generateDesign(n=10,
                     par.set = getParamSet(obj.fun), 
                     fun = lhs::randomLHS)  ## . If no design is given by the user, mlrMBO will generate a maximin Latin Hypercube Design of size 4 times the number of the black-box function’s parameters.


# i still want my favorite hyperparameters to be tested
simon_params &lt;- data.frame(max_depth = 6,
                           colsample_bytree= 0.8,
                           subsample = 0.8,
                           min_child_weight = 3,
                           eta  = 0.01,
                           gamma = 0) %&gt;% as_tibble()

#final design  is a combination of latin hypercube optimization and my own preferred set of parameters
final_design =  simon_params  %&gt;% bind_rows(des)

# bayes will have 10 additional iterations
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 10)</code></pre>
<pre class="r"><code># run this!
run = mbo(fun = obj.fun, 
          design = final_design,  
          control = control, 
          show.info = TRUE)

 write_rds( run, &quot;run.rds&quot;)</code></pre>
<pre class="r"><code>run &lt;- read_rds( &quot;run.rds&quot;)
# print a summary with run
#run
# return  best model hyperparameters using run$x
# return best log likelihood using run$y
# return all results using run$opt.path$env$path



run$opt.path$env$path  %&gt;% 
  mutate(Round = row_number()) %&gt;%
  mutate(type = case_when(
    Round==1  ~ &quot;1- hardcoded&quot;,
    Round&lt;= 11 ~ &quot;2 -design &quot;,
    TRUE ~ &quot;3 - mlrMBO optimization&quot;)) %&gt;%
  ggplot(aes(x= Round, y= y, color= type)) + 
  geom_point() +
  labs(title = &quot;mlrMBO optimization&quot;)+
  ylab(&quot;loglikelihood&quot;)</code></pre>
<p><img src="/post/2019-01-09-mlrMBO_poisson_files/figure-html/unnamed-chunk-7-1.png" width="960" style="display: block; margin: auto;" /></p>
<div id="mlrmbo-further-reading" class="section level2">
<h2>mlrMBO further reading</h2>
<ul>
<li><a href="http://rstudio-pubs-static.s3.amazonaws.com/336732_52d1b0e682634b5eae42cf86e1fc2a98.html">Xgboost using MLR package</a><br />
</li>
<li><a href="https://mlrmbo.mlr-org.com/articles/supplementary/machine_learning_with_mlrmbo.html">Vignette: https://mlrmbo.mlr-org.com/articles/supplementary/machine_learning_with_mlrmbo.html</a> , how to use lrMBO with mlr. I dont think you can do poisson regression using mlr.</li>
<li><a href="https://www.r-bloggers.com/parameter-tuning-with-mlrhyperopt/">Parameter tuning with mlrHyperopt</a><br />
</li>
<li><a href="https://www.kaggle.com/xanderhorn/train-r-ml-models-efficiently-with-mlr">Train R ML models efficiently with mlr</a></li>
</ul>
</div>
</div>
<div id="additional-example-not-recommended-bayesian-optimization-using-rbayesianoptimization" class="section level1">
<h1>Additional Example: (NOT RECOMMENDED) Bayesian optimization using <code>rBayesianOptimization</code></h1>
<p>Here I show how to do an equivalent optimization using <code>rBaysianOptimization</code>. I do not recommend using this package because it <a href="https://github.com/yanyachen/rBayesianOptimization/issues/4">recycles hyperparameters</a> and hasnt been updated on github since 2016.</p>
<p>First, we have to define a special function that return a list of two values that will be returned to the bayesian optimiser:<br />
- “Score” should be the metrics to be maximized ,<br />
- “Pred” should be the validation/cross-valiation prediction for ensembling / stacking. We can set it to 0 to save on memory.</p>
<p>This function will be named <code>xgb_cv_bayes</code> :</p>
<pre class="r"><code>xgb_cv_bayes &lt;- 
  function(max_depth=4, 
           min_child_weight=1, 
           gamma=0,
           eta=0.01,
           subsample = 0.6,
           colsample_bytree =0.6,
           nrounds = 200, 
           early_stopping_rounds = 50 ){
  set.seed(1234)
  cv &lt;- xgb.cv(params = list(
    booster = &quot;gbtree&quot;,
    eta = eta,
    max_depth = max_depth,
    min_child_weight = min_child_weight,
    gamma = gamma,
    subsample = subsample,
    colsample_bytree = colsample_bytree,
    objective = &#39;count:poisson&#39;, 
    eval_metric = &quot;poisson-nloglik&quot;),
    data = mydb_xgbmatrix,
    nround = nrounds,
    folds=  cv_folds,
    monotone_constraints = myConstraint$sens,
    prediction = FALSE,
    showsd = TRUE,
    early_stopping_rounds = 50,
    verbose = 0)
  
  list(Score = cv$evaluation_log[, max(test_poisson_nloglik_mean)],
       Pred = 0)
}</code></pre>
<p>We then launch <code>BayesianOptimization</code>, specifying:<br />
- the function to optimise (xgb_cv_bayes),<br />
- the hyperparameters (<code>bounds</code>) ,<br />
- the number of iterations, <code>n_iter</code>,</p>
<p>The optimization function needs some points for initialisation. We pass one of the two following parameters:<br />
- <code>init_points</code> , start from scratch using this number of randomly generated hyperparameter sets, or - <code>init_grid_dt</code> use the knowledge from a previous run.</p>
<div id="rbayesianoptimization-from-scratch" class="section level2">
<h2>rBayesianOptimization from scratch</h2>
<p>Here is an example “from scratch”.</p>
<pre class="r"><code>start &lt;- Sys.time()
bayesian_results &lt;- rBayesianOptimization::BayesianOptimization(
  FUN = xgb_cv_bayes,
  bounds = list(max_depth = c(2L, 10L),
                colsample_bytree = c(0.3, 1),
                subsample = c(0.3,1),
                min_child_weight = c(1L, 10L),
                eta = c(0.001, 0.03),
                gamma = c(0, 5)),
  init_grid_dt = NULL, init_points = 4, 
  n_iter = 7,
  acq = &quot;ucb&quot;, kappa = 2.576, eps = 0.0,
  verbose = TRUE)
stop &lt;- Sys.time()
stop- start
write_rds(bayesian_results, &quot;bayesian_results.rds&quot;)</code></pre>
<pre class="r"><code>bayesian_results &lt;- read_rds( &quot;bayesian_results.rds&quot;)</code></pre>
<pre class="r"><code>kable(bayesian_results$History)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">Round</th>
<th align="center">max_depth</th>
<th align="center">colsample_bytree</th>
<th align="center">subsample</th>
<th align="center">min_child_weight</th>
<th align="center">eta</th>
<th align="center">gamma</th>
<th align="center">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">5</td>
<td align="center">0.8818084</td>
<td align="center">0.7728627</td>
<td align="center">8</td>
<td align="center">0.0281948</td>
<td align="center">3.076982</td>
<td align="center">0.5131780</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">2</td>
<td align="center">0.7781656</td>
<td align="center">0.3211035</td>
<td align="center">7</td>
<td align="center">0.0199310</td>
<td align="center">4.103573</td>
<td align="center">0.5145313</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">4</td>
<td align="center">0.6018680</td>
<td align="center">0.4758529</td>
<td align="center">1</td>
<td align="center">0.0191684</td>
<td align="center">1.901145</td>
<td align="center">0.5146620</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">3</td>
<td align="center">0.9978310</td>
<td align="center">0.3976353</td>
<td align="center">9</td>
<td align="center">0.0121705</td>
<td align="center">2.959374</td>
<td align="center">0.5158113</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">4</td>
<td align="center">0.7361057</td>
<td align="center">0.7468013</td>
<td align="center">6</td>
<td align="center">0.0016526</td>
<td align="center">2.087807</td>
<td align="center">0.5175617</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">4</td>
<td align="center">0.7360925</td>
<td align="center">0.7468013</td>
<td align="center">6</td>
<td align="center">0.0016092</td>
<td align="center">2.087807</td>
<td align="center">0.5175690</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">4</td>
<td align="center">0.7360928</td>
<td align="center">0.7468013</td>
<td align="center">6</td>
<td align="center">0.0016092</td>
<td align="center">2.087807</td>
<td align="center">0.5175690</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center">4</td>
<td align="center">0.7360923</td>
<td align="center">0.7468013</td>
<td align="center">6</td>
<td align="center">0.0016093</td>
<td align="center">2.087807</td>
<td align="center">0.5175690</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="center">4</td>
<td align="center">0.7360919</td>
<td align="center">0.7468013</td>
<td align="center">6</td>
<td align="center">0.0016094</td>
<td align="center">2.087807</td>
<td align="center">0.5175690</td>
</tr>
<tr class="even">
<td align="center">10</td>
<td align="center">4</td>
<td align="center">0.7361056</td>
<td align="center">0.7468013</td>
<td align="center">6</td>
<td align="center">0.0016053</td>
<td align="center">2.087807</td>
<td align="center">0.5175697</td>
</tr>
<tr class="odd">
<td align="center">11</td>
<td align="center">4</td>
<td align="center">0.7361056</td>
<td align="center">0.7468013</td>
<td align="center">6</td>
<td align="center">0.0016055</td>
<td align="center">2.087807</td>
<td align="center">0.5175697</td>
</tr>
</tbody>
</table>
<pre class="r"><code>bayesian_results$History  %&gt;% 
  mutate(type = case_when(
    Round&lt;= 4  ~ &quot;1- init_points&quot;,
    Round&lt;= 11 ~ &quot;2 -n_iter&quot;,
    TRUE ~ &quot;wtf&quot;)) %&gt;%
  ggplot(aes(x= Round, y= Value, color= type)) + geom_point()+
  labs(title = &quot;bayesian from scratch&quot;)</code></pre>
<p><img src="/post/2019-01-09-mlrMBO_poisson_files/figure-html/unnamed-chunk-10-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="rbayesianoptimization-resuming-from-previous-runs" class="section level2">
<h2>rBayesianOptimization resuming from previous runs</h2>
<p><code>init_grid_dt</code> is a data frame with the same columns as <code>bounds</code> plus a <code>Value</code>column which correspond to the log likelihood we calculated earlier.</p>
<div id="resuming-from-random-grid" class="section level3">
<h3>… resuming from random grid</h3>
<p>Here is an example resuming from the random grid of example #1</p>
<pre class="r"><code>init_grid &lt;- random_grid_results$data %&gt;% 
  bind_rows() %&gt;% 
  add_column(Value = random_grid_results$Score)   %&gt;% 
  select(max_depth, colsample_bytree, subsample, min_child_weight, eta, gamma, Value)

kable(init_grid)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">max_depth</th>
<th align="center">colsample_bytree</th>
<th align="center">subsample</th>
<th align="center">min_child_weight</th>
<th align="center">eta</th>
<th align="center">gamma</th>
<th align="center">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8000000</td>
<td align="center">0.8000000</td>
<td align="center">3</td>
<td align="center">0.0100000</td>
<td align="center">0.000000</td>
<td align="center">0.5161770</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">0.3697140</td>
<td align="center">0.7176482</td>
<td align="center">3</td>
<td align="center">0.0447509</td>
<td align="center">0.000000</td>
<td align="center">0.5104687</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">0.7213390</td>
<td align="center">0.8263462</td>
<td align="center">5</td>
<td align="center">0.0259997</td>
<td align="center">0.000000</td>
<td align="center">0.5135350</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.3004441</td>
<td align="center">0.6424290</td>
<td align="center">4</td>
<td align="center">0.0215211</td>
<td align="center">0.000000</td>
<td align="center">0.5142750</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.4137765</td>
<td align="center">0.6237757</td>
<td align="center">7</td>
<td align="center">0.0474252</td>
<td align="center">0.000000</td>
<td align="center">0.5100350</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.5088913</td>
<td align="center">0.8314850</td>
<td align="center">3</td>
<td align="center">0.0141615</td>
<td align="center">0.000000</td>
<td align="center">0.5154890</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">0.2107123</td>
<td align="center">0.2186650</td>
<td align="center">5</td>
<td align="center">0.0446673</td>
<td align="center">0.000000</td>
<td align="center">0.5104473</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">0.5059104</td>
<td align="center">0.5817841</td>
<td align="center">8</td>
<td align="center">0.0093015</td>
<td align="center">0.000000</td>
<td align="center">0.5162917</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8957527</td>
<td align="center">0.7858510</td>
<td align="center">1</td>
<td align="center">0.0167293</td>
<td align="center">0.000000</td>
<td align="center">0.5150617</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0.4722792</td>
<td align="center">0.7541852</td>
<td align="center">9</td>
<td align="center">0.0105983</td>
<td align="center">0.000000</td>
<td align="center">0.5160767</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.5856641</td>
<td align="center">0.5820957</td>
<td align="center">4</td>
<td align="center">0.0163778</td>
<td align="center">0.000000</td>
<td align="center">0.5151200</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0.6796527</td>
<td align="center">0.8889676</td>
<td align="center">9</td>
<td align="center">0.0055361</td>
<td align="center">3.531973</td>
<td align="center">0.5169180</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.5948330</td>
<td align="center">0.5504777</td>
<td align="center">4</td>
<td align="center">0.0405373</td>
<td align="center">2.702602</td>
<td align="center">0.5111533</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">0.3489741</td>
<td align="center">0.3958378</td>
<td align="center">4</td>
<td align="center">0.0545762</td>
<td align="center">9.926841</td>
<td align="center">0.5088507</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8618987</td>
<td align="center">0.2565432</td>
<td align="center">5</td>
<td align="center">0.0487349</td>
<td align="center">6.334933</td>
<td align="center">0.5097950</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="center">0.7347734</td>
<td align="center">0.2795729</td>
<td align="center">9</td>
<td align="center">0.0498385</td>
<td align="center">2.132081</td>
<td align="center">0.5096150</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8353919</td>
<td align="center">0.4530174</td>
<td align="center">9</td>
<td align="center">0.0293165</td>
<td align="center">1.293724</td>
<td align="center">0.5129903</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.2863549</td>
<td align="center">0.6149074</td>
<td align="center">4</td>
<td align="center">0.0266050</td>
<td align="center">4.781180</td>
<td align="center">0.5134367</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.7789688</td>
<td align="center">0.7296041</td>
<td align="center">8</td>
<td align="center">0.0506522</td>
<td align="center">9.240745</td>
<td align="center">0.5095087</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">0.5290195</td>
<td align="center">0.5254641</td>
<td align="center">10</td>
<td align="center">0.0382960</td>
<td align="center">5.987610</td>
<td align="center">0.5115177</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">0.8567570</td>
<td align="center">0.9303007</td>
<td align="center">5</td>
<td align="center">0.0412834</td>
<td align="center">9.761707</td>
<td align="center">0.5110477</td>
</tr>
</tbody>
</table>
<pre class="r"><code>bayesian_results_continued_from_randomgrid &lt;- 
  BayesianOptimization(xgb_cv_bayes,
                       bounds = list(max_depth = c(2L, 10L),
                                     colsample_bytree = c(0.3, 1),
                                     subsample = c(0.3,1),
                                     min_child_weight = c(1L, 10L),
                                     eta = c(0.001, 0.03),
                                     gamma = c(0, 5)),
                       init_grid_dt = init_grid, init_points = 0, 
                       n_iter = 5,
                       acq = &quot;ucb&quot;, kappa = 2.576, eps = 0.0,
                       verbose = TRUE)

write_rds(bayesian_results_continued_from_randomgrid,
          &quot;bayesian_results_continued_from_randomgrid.rds&quot;)</code></pre>
<pre class="r"><code>bayesian_results_continued_from_randomgrid &lt;- read_rds(
  &quot;bayesian_results_continued_from_randomgrid.rds&quot;)</code></pre>
</div>
<div id="resuming-from-previous-rbayesianoptimization" class="section level3">
<h3>… resuming from previous rBayesianOptimization</h3>
<p>We can also resume from a brevious rBayesianOptimization run using the <code>$History</code> value it returns. In the example below, we will resume from the previous bayesian search that was itself build upon the random grid search.</p>
<pre class="r"><code>init_grid &lt;- bayesian_results_continued_from_randomgrid$History %&gt;% select(-Round)
kable(init_grid)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">max_depth</th>
<th align="center">colsample_bytree</th>
<th align="center">subsample</th>
<th align="center">min_child_weight</th>
<th align="center">eta</th>
<th align="center">gamma</th>
<th align="center">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8000000</td>
<td align="center">0.8000000</td>
<td align="center">3</td>
<td align="center">0.0100000</td>
<td align="center">0.0000000</td>
<td align="center">0.5161770</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">0.3697140</td>
<td align="center">0.7176482</td>
<td align="center">3</td>
<td align="center">0.0447509</td>
<td align="center">0.0000000</td>
<td align="center">0.5104687</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">0.7213390</td>
<td align="center">0.8263462</td>
<td align="center">5</td>
<td align="center">0.0259997</td>
<td align="center">0.0000000</td>
<td align="center">0.5135350</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.3004441</td>
<td align="center">0.6424290</td>
<td align="center">4</td>
<td align="center">0.0215211</td>
<td align="center">0.0000000</td>
<td align="center">0.5142750</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.4137765</td>
<td align="center">0.6237757</td>
<td align="center">7</td>
<td align="center">0.0474252</td>
<td align="center">0.0000000</td>
<td align="center">0.5100350</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.5088913</td>
<td align="center">0.8314850</td>
<td align="center">3</td>
<td align="center">0.0141615</td>
<td align="center">0.0000000</td>
<td align="center">0.5154890</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">0.2107123</td>
<td align="center">0.2186650</td>
<td align="center">5</td>
<td align="center">0.0446673</td>
<td align="center">0.0000000</td>
<td align="center">0.5104473</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">0.5059104</td>
<td align="center">0.5817841</td>
<td align="center">8</td>
<td align="center">0.0093015</td>
<td align="center">0.0000000</td>
<td align="center">0.5162917</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8957527</td>
<td align="center">0.7858510</td>
<td align="center">1</td>
<td align="center">0.0167293</td>
<td align="center">0.0000000</td>
<td align="center">0.5150617</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0.4722792</td>
<td align="center">0.7541852</td>
<td align="center">9</td>
<td align="center">0.0105983</td>
<td align="center">0.0000000</td>
<td align="center">0.5160767</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.5856641</td>
<td align="center">0.5820957</td>
<td align="center">4</td>
<td align="center">0.0163778</td>
<td align="center">0.0000000</td>
<td align="center">0.5151200</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0.6796527</td>
<td align="center">0.8889676</td>
<td align="center">9</td>
<td align="center">0.0055361</td>
<td align="center">3.5319727</td>
<td align="center">0.5169180</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.5948330</td>
<td align="center">0.5504777</td>
<td align="center">4</td>
<td align="center">0.0405373</td>
<td align="center">2.7026015</td>
<td align="center">0.5111533</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">0.3489741</td>
<td align="center">0.3958378</td>
<td align="center">4</td>
<td align="center">0.0545762</td>
<td align="center">9.9268406</td>
<td align="center">0.5088507</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8618987</td>
<td align="center">0.2565432</td>
<td align="center">5</td>
<td align="center">0.0487349</td>
<td align="center">6.3349326</td>
<td align="center">0.5097950</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="center">0.7347734</td>
<td align="center">0.2795729</td>
<td align="center">9</td>
<td align="center">0.0498385</td>
<td align="center">2.1320814</td>
<td align="center">0.5096150</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.8353919</td>
<td align="center">0.4530174</td>
<td align="center">9</td>
<td align="center">0.0293165</td>
<td align="center">1.2937235</td>
<td align="center">0.5129903</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">0.2863549</td>
<td align="center">0.6149074</td>
<td align="center">4</td>
<td align="center">0.0266050</td>
<td align="center">4.7811803</td>
<td align="center">0.5134367</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.7789688</td>
<td align="center">0.7296041</td>
<td align="center">8</td>
<td align="center">0.0506522</td>
<td align="center">9.2407447</td>
<td align="center">0.5095087</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">0.5290195</td>
<td align="center">0.5254641</td>
<td align="center">10</td>
<td align="center">0.0382960</td>
<td align="center">5.9876097</td>
<td align="center">0.5115177</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">0.8567570</td>
<td align="center">0.9303007</td>
<td align="center">5</td>
<td align="center">0.0412834</td>
<td align="center">9.7617069</td>
<td align="center">0.5110477</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">0.9990990</td>
<td align="center">0.8070583</td>
<td align="center">6</td>
<td align="center">0.0071162</td>
<td align="center">0.9403848</td>
<td align="center">0.5166537</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">1.0000000</td>
<td align="center">0.3560251</td>
<td align="center">2</td>
<td align="center">0.0010000</td>
<td align="center">0.0000000</td>
<td align="center">0.5176700</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">0.3000000</td>
<td align="center">0.3838126</td>
<td align="center">1</td>
<td align="center">0.0010000</td>
<td align="center">5.0000000</td>
<td align="center">0.5176697</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">0.6074937</td>
<td align="center">0.4582376</td>
<td align="center">9</td>
<td align="center">0.0021338</td>
<td align="center">1.5529544</td>
<td align="center">0.5174817</td>
</tr>
<tr class="even">
<td align="center">10</td>
<td align="center">1.0000000</td>
<td align="center">0.3000000</td>
<td align="center">10</td>
<td align="center">0.0010000</td>
<td align="center">5.0000000</td>
<td align="center">0.5176700</td>
</tr>
</tbody>
</table>
<pre class="r"><code>bayesian_results_continued_from_bayesian &lt;- 
  BayesianOptimization(xgb_cv_bayes,
                       bounds = list(max_depth = c(2L, 10L),
                                     colsample_bytree = c(0.3, 1),
                                     subsample = c(0.3,1),
                                     min_child_weight = c(1L, 10L),
                                     eta = c(0.001, 0.03),
                                     gamma = c(0, 5)),
                       init_grid_dt =init_grid , init_points = 0,
                       n_iter = 5,
                       acq = &quot;ucb&quot;, kappa = 2.576, eps = 0.0,
                       verbose = TRUE)

write_rds(bayesian_results_continued_from_bayesian, &quot;bayesian_results_continued_from_bayesian.rds&quot;)</code></pre>
<pre class="r"><code>bayesian_results_continued_from_bayesian &lt;- read_rds(
  &quot;bayesian_results_continued_from_bayesian.rds&quot;)</code></pre>
<pre class="r"><code>bayesian_results_continued_from_bayesian$History  %&gt;% 
  mutate(type = case_when(
    Round==1  ~ &quot;1- hardcoded&quot;,
    Round&lt;= 21 ~ &quot;2 -random grid &quot;,
    Round &lt;= 26 ~ &quot;3 - rbayesianoptimization run #1 resuming from random grid&quot;,
    TRUE ~ &quot;4 - rbayesianoptimization run #2 resuming from bayesian run 1&quot;)) %&gt;%
  ggplot(aes(x= Round, y= Value, color= type)) + 
  geom_point()+ 
  labs(title = &quot;rBayesianOptimization&quot;)+
  ylab(&quot;loglikelihood&quot;)</code></pre>
<p><img src="/post/2019-01-09-mlrMBO_poisson_files/figure-html/results-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>The table below shows the loglikelihood and hyperparameters for the hardcoded <code>simon_param</code> model, and the best models found by random search, mlrMBO and rBayesianOptimisation.</p>
<p>Bayesian optimization using mlrMBO and rBayesianOptimization both yield the best results. The gain in loglikelihood above the “hardcoded” <code>simon_param</code> or the random search isnt that great, however, so it may not be necessary to implement <code>mlrMBO</code> in a non-kaggle setting.</p>
<pre class="r"><code>hardcoded &lt;- random_grid_results[1,] %&gt;%
  pull(data) %&gt;% .[[1]] %&gt;%
  mutate(type = &quot;hardcoded&quot;) %&gt;%
  mutate(loglikelihood= random_grid_results[1,&quot;Score&quot;] %&gt;% as.numeric())

best_random &lt;- random_grid_results %&gt;% 
  filter(Score == max(Score)) %&gt;% pull(data) %&gt;% .[[1]] %&gt;% 
  mutate(type = &quot;random_grid&quot;) %&gt;%
  mutate(loglikelihood = max(random_grid_results$Score))
  

best_mlrMBO &lt;- run$opt.path$env$path  %&gt;% 
  filter(y == max(y))  %&gt;%
  mutate(type = &quot;mlrMBO&quot;) %&gt;%
  mutate(loglikelihood= run$y )

best_rBayesianOptimization &lt;- 
  bayesian_results_continued_from_bayesian$History %&gt;%
  filter(Value == max(Value)) %&gt;%
  mutate(type = &quot;rBayesianOptimization&quot;) %&gt;%
  mutate(loglikelihood = max(bayesian_results_continued_from_bayesian$History$Value))


hardcoded %&gt;% 
  bind_rows(best_random) %&gt;% 
  bind_rows(best_mlrMBO) %&gt;%
  bind_rows(best_rBayesianOptimization) %&gt;%
  select(type,loglikelihood, max_depth, colsample_bytree, subsample,min_child_weight,eta, gamma) %&gt;% kable</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">type</th>
<th align="center">loglikelihood</th>
<th align="center">max_depth</th>
<th align="center">colsample_bytree</th>
<th align="center">subsample</th>
<th align="center">min_child_weight</th>
<th align="center">eta</th>
<th align="center">gamma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">hardcoded</td>
<td align="center">0.5161770</td>
<td align="center">6</td>
<td align="center">0.8000000</td>
<td align="center">0.8000000</td>
<td align="center">3</td>
<td align="center">0.0100000</td>
<td align="center">0.000000</td>
</tr>
<tr class="even">
<td align="center">random_grid</td>
<td align="center">0.5169180</td>
<td align="center">3</td>
<td align="center">0.6796527</td>
<td align="center">0.8889676</td>
<td align="center">9</td>
<td align="center">0.0055361</td>
<td align="center">3.531973</td>
</tr>
<tr class="odd">
<td align="center">mlrMBO</td>
<td align="center">0.5176700</td>
<td align="center">7</td>
<td align="center">0.7271841</td>
<td align="center">0.7174480</td>
<td align="center">6</td>
<td align="center">0.0010014</td>
<td align="center">3.365226</td>
</tr>
<tr class="even">
<td align="center">rBayesianOptimization</td>
<td align="center">0.5176707</td>
<td align="center">10</td>
<td align="center">0.3000000</td>
<td align="center">0.4696255</td>
<td align="center">2</td>
<td align="center">0.0010000</td>
<td align="center">5.000000</td>
</tr>
</tbody>
</table>
</div>
