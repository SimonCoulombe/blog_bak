---
title: "Bayesian optimization of xgboost poisson regression in R"
author: "Simon Coulombe"
date: 2019-01-09
slug: "bayesian"
output:
  html_document:
    code_folding: hide
    number_sections: true
    theme: simplex
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    editor_options:   
      chunk_output_type: console
categories: ["R"]
tags: ["bayesian", "optimization", "rstats", "mlrMBO", "poisson", "rBayesianOptimization"]
---


```{r setup, include =F}

# TODO : valider ceci : Chunk options must be written in one line; no line breaks are allowed inside chunk options;
# https://yihui.name/knitr/options/

knitr::opts_chunk$set(collapse = FALSE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE,
                      fig.align= "center",
                      fig.width = 10,
                      highlight = TRUE,
                      cache = FALSE,
                      cache.lazy = FALSE) # fixes long vector not supported quand on cache des gros éléments https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-error-in-rmd-but-not-in-r-script
```

# Introduction  

In this document, I will show how to optimize the xgboost hyperparameters for a poisson regression in R using the `mlrMBO` package. We will model the number of auto insurance claims based on characteristics of the car and driver, while offsetting for exposure.   The data comes from the `insuranceData` package.

We will demonstrate the code for a random search, a bayesian optimization using the `mlrMBO`package and also show how to perform a bayesian optimization using the deprecated (?)  `rBayesianOptimization` package.  

The `mlrMBO`  package appears to be well maintained.  It is normally used in combination with the `mlr` package, but I couldnt find a way to use `mlr` to model a poisson regression using xgboost.

I also tried the `rBayesianOptimization` package after being inspired by this post from  [Max Kuhn](http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html) in 2016 the example from `rBayesianOptimization` 's help.   I do not recommend using this package because it  [sometimes recycles hyperparameters](https://github.com/yanyachen/rBayesianOptimization/issues/4) and hasnt been updated on github since 2016.

In this document, we will use the , `xgboost`, `tidyverse`, `mlrMBO`,  `insuranceData` and `rBayesianOptimization` packages.  


```{r librairies}
library(xgboost)
library(insuranceData) # example dataset https://cran.r-project.org/web/packages/insuranceData/insuranceData.pdf
library(tidyverse) # for data wrangling
library(rBayesianOptimization) # to create cv folds and for bayesian optimisation
library(mlrMBO)  # for bayesian optimisation
library(DataExplorer) # for summarising databases
library(skimr) # for summarising databases
library(purrr) # to evaluate the loglikelihood of each parameter set in the random grid search

``` 


# Preparing the data

First, we load the dataCar data from the `insuranceData` package.  It contains 67856 one-year vehicle insurance policies taken out in 2004 or 2005.   

The dependent variable is  `numclaims`, which represents the number of claims.    

The `exposure` variable  represents the "number of year of exposure" and is used as the offset variable.  Its value is bounded between 0 and 1.   

Finally, the independent variables are as follow:  

  * `veh_value`, the vehicle value in tens of thousand of dollars,  
  * `veh_body`, y vehicle body, coded as BUS CONVT COUPE HBACK HDTOP MCARA MIBUS PANVN RDSTR SEDAN STNWG TRUCK UTE,  
  * `veh_age`, 1 (youngest), 2, 3, 4,   
  * `gender`, a factor with levels F M,   
  * `area` a factor with levels A B C D E F,   
  * `agecat` 1 (youngest), 2, 3, 4, 5, 6  

Insurance ratemaking often requires monotonous relationships.  In our case, we will arbitrarily force the number of claims to be non-increasing with the age of the vehicle.

Below, we import the data, one-hot encode dummy variables for xgboost consumption, set the offset, set the constraints and define the 3 folds for cross-validation.


```{r prep_dataCar}

# load insurance data
data(dataCar)
mydb <- dataCar %>% select(numclaims, exposure, veh_value, veh_body,
                           veh_age, gender, area, agecat)

label_var <- "numclaims"  
offset_var <- "exposure"
feature_vars <- mydb %>% 
  select(-one_of(c(label_var, offset_var))) %>% 
  colnames()

skimr::skim(mydb ) %>% 
  skimr::kable()
```


```{r}

# one hot encoding of categorical (factor) data
myformula <- paste0( "~", paste0( feature_vars, collapse = " + ") ) %>% as.formula()
dummyFier <- caret::dummyVars(myformula, data=mydb, fullRank = TRUE)
dummyVars.df <- predict(dummyFier,newdata = mydb)
mydb_dummy <- cbind(mydb %>% select(one_of(c(label_var, offset_var))), 
                    dummyVars.df)
rm(myformula, dummyFier, dummyVars.df)


# get  list the column names of the db with the dummy variables
feature_vars_dummy <-  mydb_dummy  %>% select(-one_of(c(label_var, offset_var))) %>% colnames()

# create xgb.matrix for xgboost consumption
mydb_xgbmatrix <- xgb.DMatrix(
  data = mydb_dummy %>% select(feature_vars_dummy) %>% as.matrix, 
  label = mydb_dummy %>% pull(label_var),
  missing = "NAN")


#base_margin: apply exposure offset 
setinfo(mydb_xgbmatrix,"base_margin", mydb %>% pull(offset_var) %>% log() )

# a fake constraint, just to show how it is done.  Here we force "the older the car, the less likely are claims"
myConstraint   <- data_frame(Variable = feature_vars_dummy) %>%
  mutate(sens = ifelse(Variable == "veh_age", -1, 0))

# random folds for xgb.cv
cv_folds = rBayesianOptimization::KFold(mydb_dummy$numclaims,
                                        nfolds= 3,
                                        stratified = TRUE,
                                        seed= 0)
```


# Example 1: Optimize hyperparameters using a random search   (non bayesian)

We will start with a quick example of random search.  

I don't use caret for the random search  [because it has a hard time with poisson regression](https://github.com/topepo/caret/issues/861).

I will  generate 20 random sets of hyperparameters.  I will force the gamma's half the  random sets to be equal to 0.   I will also hardcode an extra set of parameters named `simon_params` because they are often a good starting point.

```{r random_grid_search, echo=T}
# generate hard coded parameters
simon_params <- data.frame(max_depth = 6,
                           colsample_bytree= 0.8,
                           subsample = 0.8,
                           min_child_weight = 3,
                           eta  = 0.01,
                           gamma = 0,
                           nrounds = 200) %>% as_tibble()

# generate 20 random models
how_many_models <- 20
max_depth <-data.frame(max_depth = floor(runif(how_many_models)*5 )+3)  # 1 à 4
colsample_bytree <- data.frame(colsample_bytree =runif(how_many_models)*0.8 +0.2)  # 0.2 à 1
subsample <- data.frame(subsample =runif(how_many_models)*0.8 +0.2) # 0.2 à 1
min_child_weight <- data.frame(min_child_weight = floor(runif(how_many_models)*10 )+1) # 1 à 10
eta <- data.frame(eta = runif(how_many_models)*0.06 +0.002) # 0.002 à 0.062
gamma <-  data.frame(gamma =c(rep(0,how_many_models/2), runif(how_many_models/2)*10)) # 0 à 10
nrounds <- data.frame(nrounds = rep(2e2,how_many_models)) # max 200

random_grid <-max_depth %>%
  bind_cols(colsample_bytree ) %>%
  bind_cols(subsample) %>%
  bind_cols(min_child_weight) %>%
  bind_cols(eta) %>%
  bind_cols(gamma) %>%
  bind_cols(nrounds)  %>% as_tibble()

# combine random and hardcoded parameters
df.params <- 
  simon_params %>%  bind_rows(random_grid) %>%
  mutate(rownum = row_number(),
         rownumber = row_number())


list_of_param_sets <- df.params %>% nest(-rownum)

```

Here are the hyperparameters that will be tested:
 
```{r}
kable(df.params)
```

Evaluate these 21 models with 3 folds and retourn the loglikelihood of each models:  



```{r, eval=F}

start <- Sys.time()

random_grid_results <- list_of_param_sets %>% 
  mutate(booster = map(data, function(X){
    message(paste0("model #", X$rownumber), 
            " eta = ", X$eta,
            " max.depth = ", X$max_depth, 
            " min_child_weigth = ", X$min_child_weight,
            " subsample = ", X$subsample, 
            " colsample_bytree = ", X$colsample_bytree, 
            ", gamma = ",  X$gamma," , 
            nrounds = ", X$nrounds)
    set.seed(1234)
    
    cv <- xgb.cv(params = list(
      booster = "gbtree",
      eta = X$eta,
      max_depth = X$max_depth,
      min_child_weight = X$min_child_weight,
      gamma = X$gamma,
      subsample = X$subsample,
      colsample_bytree = X$colsample_bytree,
      objective = 'count:poisson', 
      eval_metric = "poisson-nloglik"),
      data = mydb_xgbmatrix,
      nround = X$nrounds,
      folds=  cv_folds,
      monotone_constraints = myConstraint$sens,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 50,
      verbose = 0)
    
    function_return <- 
      list(
        Score = cv$evaluation_log[, max(test_poisson_nloglik_mean)], # l'itération qui a la plus haute moyenne de nloglik sur toutes les cv folds
        Pred = 0)
    message(paste0("Score :", function_return$Score))
    return(function_return)})) %>%
  mutate(Score =  pmap(list(booster), function(X){X$Score })%>% unlist())

write_rds(random_grid_results, "random_grid_results.rds")
stop <- Sys.time()
stop- start # 4 minutes on 32 cores
```

```{r}
random_grid_results <- read_rds( "random_grid_results.rds")

#kable(random_grid_results)

random_grid_results %>%
  mutate( hardcoded = ifelse(rownum ==1,TRUE,FALSE)) %>%
  ggplot(aes( x = rownum, y = Score, color = hardcoded)) + 
  geom_point() +
  labs(title = "random grid search")+
  ylab("loglikelihood")
```

# Example #2 Bayesian optimization using `mlrMBO`

This tutorial builds on the [mlrMBO vignette](https://mlrmbo.mlr-org.com/articles/mlrMBO.html)

First, we need to define the objective function that the bayesian search will try to maximise.  In this case we want to maximise the log likelihood of the out of fold predictions.

```{r}
# objective function: we want to maximise the log likelihood by tuning most parameters
obj.fun  <- smoof::makeSingleObjectiveFunction(
  name = "xgb_cv_bayes",
  fn =   function(x){
    set.seed(1234)
    cv <- xgb.cv(params = list(
      booster = "gbtree",
      eta = x["eta"],
      max_depth = x["max_depth"],
      min_child_weight = x["min_child_weight"],
      gamma = x["gamma"],
      subsample =x["subsample"],
      colsample_bytree = x["colsample_bytree"],
      objective = 'count:poisson', 
      eval_metric = "poisson-nloglik"),
      data = mydb_xgbmatrix,
      nround = 30,
      folds=  cv_folds,
      monotone_constraints = myConstraint$sens,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 10,
      verbose = 0)
    
    cv$evaluation_log[, max(test_poisson_nloglik_mean)]
  },
  par.set = makeParamSet(
    makeNumericParam("eta", lower = 0.001, upper = 0.05),
    makeNumericParam("gamma", lower = 0, upper = 5),
    makeIntegerParam("max_depth", lower= 1, upper = 10),
    makeIntegerParam("min_child_weight", lower= 1, upper = 10),
    makeNumericParam("subsample", lower = 0.2, upper = 1),
    makeNumericParam("colsample_bytree", lower = 0.2, upper = 1)
  ),
  minimize = FALSE
)

# generate an optimal design with only 10  points
des = generateDesign(n=10,
                     par.set = getParamSet(obj.fun), 
                     fun = lhs::randomLHS)  ## . If no design is given by the user, mlrMBO will generate a maximin Latin Hypercube Design of size 4 times the number of the black-box function’s parameters.


# i still want my favorite hyperparameters to be tested
simon_params <- data.frame(max_depth = 6,
                           colsample_bytree= 0.8,
                           subsample = 0.8,
                           min_child_weight = 3,
                           eta  = 0.01,
                           gamma = 0) %>% as_tibble()

#final design  is a combination of latin hypercube optimization and my own preferred set of parameters
final_design =  simon_params  %>% bind_rows(des)

# bayes will have 10 additional iterations
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 10)
```

```{r, eval =F}
# run this!
run = mbo(fun = obj.fun, 
          design = final_design,  
          control = control, 
          show.info = TRUE)

 write_rds( run, "run.rds")
```

```{r}
run <- read_rds( "run.rds")
# print a summary with run
#run
# return  best model hyperparameters using run$x
# return best log likelihood using run$y
# return all results using run$opt.path$env$path



run$opt.path$env$path  %>% 
  mutate(Round = row_number()) %>%
  mutate(type = case_when(
    Round==1  ~ "1- hardcoded",
    Round<= 11 ~ "2 -design ",
    TRUE ~ "3 - mlrMBO optimization")) %>%
  ggplot(aes(x= Round, y= y, color= type)) + 
  geom_point() +
  labs(title = "mlrMBO optimization")+
  ylab("loglikelihood")


```


## mlrMBO further reading


  * [Xgboost using MLR package](http://rstudio-pubs-static.s3.amazonaws.com/336732_52d1b0e682634b5eae42cf86e1fc2a98.html)  
  * [Vignette: https://mlrmbo.mlr-org.com/articles/supplementary/machine_learning_with_mlrmbo.html](https://mlrmbo.mlr-org.com/articles/supplementary/machine_learning_with_mlrmbo.html) , how to use lrMBO with mlr. I dont think you can do poisson regression using mlr.
  * [Parameter tuning with mlrHyperopt](https://www.r-bloggers.com/parameter-tuning-with-mlrhyperopt/)  
  * [Train R ML models efficiently with mlr](https://www.kaggle.com/xanderhorn/train-r-ml-models-efficiently-with-mlr)  
  
# Additional Example: (NOT RECOMMENDED) Bayesian optimization  using `rBayesianOptimization`


Here I show how to do an equivalent optimization using `rBaysianOptimization`.  I do not recommend using this package because it  [recycles hyperparameters](https://github.com/yanyachen/rBayesianOptimization/issues/4) and hasnt been updated on github since 2016.

First, we have to define a special function that return a list of two values that will be returned to the bayesian optimiser:  
  - "Score" should be the metrics to be maximized ,  
  - "Pred" should be the validation/cross-valiation prediction for ensembling / stacking.  We can set it to 0 to save on memory.

This function will be named  `xgb_cv_bayes` : 
```{r}
xgb_cv_bayes <- 
  function(max_depth=4, 
           min_child_weight=1, 
           gamma=0,
           eta=0.01,
           subsample = 0.6,
           colsample_bytree =0.6,
           nrounds = 200, 
           early_stopping_rounds = 50 ){
  set.seed(1234)
  cv <- xgb.cv(params = list(
    booster = "gbtree",
    eta = eta,
    max_depth = max_depth,
    min_child_weight = min_child_weight,
    gamma = gamma,
    subsample = subsample,
    colsample_bytree = colsample_bytree,
    objective = 'count:poisson', 
    eval_metric = "poisson-nloglik"),
    data = mydb_xgbmatrix,
    nround = nrounds,
    folds=  cv_folds,
    monotone_constraints = myConstraint$sens,
    prediction = FALSE,
    showsd = TRUE,
    early_stopping_rounds = 50,
    verbose = 0)
  
  list(Score = cv$evaluation_log[, max(test_poisson_nloglik_mean)],
       Pred = 0)
}
```

We then launch `BayesianOptimization`, specifying:    
  - the function to optimise (xgb_cv_bayes),  
  - the hyperparameters (`bounds`) ,  
  - the number of iterations, `n_iter`, 
  

The optimization function needs some points for initialisation.  We pass one of the two following parameters:   
  - `init_points` , start from scratch using this number of randomly generated hyperparameter sets, or 
  - `init_grid_dt`  use the knowledge from a previous run.


##  rBayesianOptimization from scratch  
Here is an example  "from scratch". 

```{r bayes_scratch, collapse = FALSE, eval = F}

start <- Sys.time()
bayesian_results <- rBayesianOptimization::BayesianOptimization(
  FUN = xgb_cv_bayes,
  bounds = list(max_depth = c(2L, 10L),
                colsample_bytree = c(0.3, 1),
                subsample = c(0.3,1),
                min_child_weight = c(1L, 10L),
                eta = c(0.001, 0.03),
                gamma = c(0, 5)),
  init_grid_dt = NULL, init_points = 4, 
  n_iter = 7,
  acq = "ucb", kappa = 2.576, eps = 0.0,
  verbose = TRUE)
stop <- Sys.time()
stop- start
write_rds(bayesian_results, "bayesian_results.rds")
```

```{r}
bayesian_results <- read_rds( "bayesian_results.rds")
```



```{r}

kable(bayesian_results$History)

bayesian_results$History  %>% 
  mutate(type = case_when(
    Round<= 4  ~ "1- init_points",
    Round<= 11 ~ "2 -n_iter",
    TRUE ~ "wtf")) %>%
  ggplot(aes(x= Round, y= Value, color= type)) + geom_point()+
  labs(title = "bayesian from scratch")


```

## rBayesianOptimization resuming from previous runs

 `init_grid_dt` is a data frame with the same columns as `bounds` plus a `Value`column which correspond to the log likelihood we calculated earlier.  
 
 
### ... resuming from random grid

Here is an example resuming from the random grid of example #1

```{r bayes_continued_randomgrid, collapse = FALSE}

init_grid <- random_grid_results$data %>% 
  bind_rows() %>% 
  add_column(Value = random_grid_results$Score)   %>% 
  select(max_depth, colsample_bytree, subsample, min_child_weight, eta, gamma, Value)

kable(init_grid)
```

```{r, eval= F}

bayesian_results_continued_from_randomgrid <- 
  BayesianOptimization(xgb_cv_bayes,
                       bounds = list(max_depth = c(2L, 10L),
                                     colsample_bytree = c(0.3, 1),
                                     subsample = c(0.3,1),
                                     min_child_weight = c(1L, 10L),
                                     eta = c(0.001, 0.03),
                                     gamma = c(0, 5)),
                       init_grid_dt = init_grid, init_points = 0, 
                       n_iter = 5,
                       acq = "ucb", kappa = 2.576, eps = 0.0,
                       verbose = TRUE)

write_rds(bayesian_results_continued_from_randomgrid,
          "bayesian_results_continued_from_randomgrid.rds")

```

```{r}
bayesian_results_continued_from_randomgrid <- read_rds(
  "bayesian_results_continued_from_randomgrid.rds")

```


### ... resuming from previous rBayesianOptimization

We can also resume from a brevious rBayesianOptimization run using the `$History` value it returns.  In the example below, we will resume from the previous bayesian search that was itself build upon the random grid search.



```{r, collapse = FALSE}
init_grid <- bayesian_results_continued_from_randomgrid$History %>% select(-Round)
kable(init_grid)
```

```{r bayes_continued_bayesian, collapse= FALSE,eval= F}
bayesian_results_continued_from_bayesian <- 
  BayesianOptimization(xgb_cv_bayes,
                       bounds = list(max_depth = c(2L, 10L),
                                     colsample_bytree = c(0.3, 1),
                                     subsample = c(0.3,1),
                                     min_child_weight = c(1L, 10L),
                                     eta = c(0.001, 0.03),
                                     gamma = c(0, 5)),
                       init_grid_dt =init_grid , init_points = 0,
                       n_iter = 5,
                       acq = "ucb", kappa = 2.576, eps = 0.0,
                       verbose = TRUE)

write_rds(bayesian_results_continued_from_bayesian, "bayesian_results_continued_from_bayesian.rds")
```

```{r}
bayesian_results_continued_from_bayesian <- read_rds(
  "bayesian_results_continued_from_bayesian.rds")
```


```{r results}
bayesian_results_continued_from_bayesian$History  %>% 
  mutate(type = case_when(
    Round==1  ~ "1- hardcoded",
    Round<= 21 ~ "2 -random grid ",
    Round <= 26 ~ "3 - rbayesianoptimization run #1 resuming from random grid",
    TRUE ~ "4 - rbayesianoptimization run #2 resuming from bayesian run 1")) %>%
  ggplot(aes(x= Round, y= Value, color= type)) + 
  geom_point()+ 
  labs(title = "rBayesianOptimization")+
  ylab("loglikelihood")
```


# Conclusion

The table below shows the loglikelihood and hyperparameters for the hardcoded `simon_param` model, and the best models found by random search, mlrMBO and rBayesianOptimisation.  


Bayesian optimization using mlrMBO and rBayesianOptimization both yield the best results.  The gain in loglikelihood above the "hardcoded" `simon_param` or the random search isnt that great, however, so it may not be necessary to implement `mlrMBO` in a non-kaggle setting.

```{r}

hardcoded <- random_grid_results[1,] %>%
  pull(data) %>% .[[1]] %>%
  mutate(type = "hardcoded") %>%
  mutate(loglikelihood= random_grid_results[1,"Score"] %>% as.numeric())

best_random <- random_grid_results %>% 
  filter(Score == max(Score)) %>% pull(data) %>% .[[1]] %>% 
  mutate(type = "random_grid") %>%
  mutate(loglikelihood = max(random_grid_results$Score))
  

best_mlrMBO <- run$opt.path$env$path  %>% 
  filter(y == max(y))  %>%
  mutate(type = "mlrMBO") %>%
  mutate(loglikelihood= run$y )

best_rBayesianOptimization <- 
  bayesian_results_continued_from_bayesian$History %>%
  filter(Value == max(Value)) %>%
  mutate(type = "rBayesianOptimization") %>%
  mutate(loglikelihood = max(bayesian_results_continued_from_bayesian$History$Value))


hardcoded %>% 
  bind_rows(best_random) %>% 
  bind_rows(best_mlrMBO) %>%
  bind_rows(best_rBayesianOptimization) %>%
  select(type,loglikelihood, max_depth, colsample_bytree, subsample,min_child_weight,eta, gamma) %>% kable

```











